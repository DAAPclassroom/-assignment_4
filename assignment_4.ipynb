{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4\n",
    "\n",
    "Due: **10:00 28. May 2024**\n",
    "\n",
    "Discussion: **12:00 11. June 2024**\n",
    "\n",
    "**Online submission** at via github classroom  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Kendall's tau coefficient [20 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, the Kendall rank correlation coefficient, commonly referred to as Kendall’s tau coefficient (after the Greek letter $\\tau$), is a statistic used to measure the ordinal association between two measured quantities. A tau test is a non-parametric hypothesis test for statistical dependence based on the tau coefficient. Spearman’s rank correlation is satisfactory for testing a null hypothesis of independence between two variables but it is difficult to interpret when the null hypothesis is rejected. Kendall’s rank correlation improves upon this by reflecting the strength of the dependence between the variables being compared.\n",
    "\n",
    "Consider two samples, $x$ and $y$, each of size $n$. The total number of possible pairings of $x$ with $y$ observations is $\\frac{n(n-1)}{2}$. Now consider pairs of ordered observations $(x_i, y_i)$ and $(x_j, y_j)$ with $i=1,\\ldots n-1$ and $j=i+1,\\ldots, n$. Then $x_1 \\leq x_2 \\leq \\ldots \\leq x_n$. Now pair $1$ is compared with every other pair $(2,3,,,n)$, Pair $2$ with all following pairs $(3,4,,,n)$ and so on. In total $\\frac{n(n-1)}{2}$ pairs are compared. If for a pair:\n",
    "\n",
    "- $x_i<x_j$ and $y_i<y_j$, it is concordant\n",
    "- $x_i<x_j$ and $y_i>y_j$, it is disconcordant\n",
    "- $x_i \\ne x_j$ and $y_i=y_j$, it has a binding in $Y$\n",
    "- $x_i = x_j$ and $y_i\\ne y_j$, it has a binding in $X$\n",
    "- $x_i = x_j$ and $y_i=y_j$, it has a binding in $X$ and $Y$\n",
    "\n",
    "\\newpage\n",
    "\n",
    "The number of pairs that are:\n",
    "\n",
    "- concordant are $C$\n",
    "- disconcordant are $D$\n",
    "- having a binding in $Y$ are $T_Y$\n",
    "- having a binding in $X$ are $T_X$\n",
    "- having a binding in $X$ and $Y$ are $T_{XY}$\n",
    "\n",
    "Kendall's tau value compares the number of concordant and disconcordant pairs:\n",
    "\n",
    "$\\tau = \\dfrac{C-D}{\\sqrt{(C+D+T_X)(C+D+T_Y)}}$\n",
    "\n",
    "The denominator is the total number of pair combinations, so the coefficient must be in the range $-1 \\leq \\tau \\leq 1$. If the agreement between the two rankings is perfect (when the two rankings are the same) the coefficient has value $1$. If the disagreement between the two rankings is perfect (when one ranking is the reverse of the other) the coefficient has value $-1$.\n",
    "\n",
    "Use the data file `Data/hubble.dat`, which includes dataset $x$ in the first column and dataset $y$ in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau, spearmanr, pearsonr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Take the data and reorder it such that $x_1 \\leq x_2 \\leq \\ldots \\leq x_n$. **5 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Compute the number of concordant pairs $C$, disconcordant pairs $D$, bound pairs $T_X$, $T_Y$, and $T_{XY}$. **5 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Compute $\\tau$. **5 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** How should the derived $\\tau$ be interpreted (what does it tell about the two datasets)? Plot the dataset $x$ vs. $y$ and compare it with $\\tau$ **5 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Permutation tests [40 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise uses the data file `Data/4point2.dat`. The data in the file are 20 uncorrelated $(x,y)$ pairs, followed by 20 correlated pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Take the uncorrelated pairs $(x,y)$ and compute the values for the Kendall and Spearman rank correlation coefficients and the Peason/Fischer r correlation coefficient. Why are they different? **10 Points**\n",
    " \n",
    " > (This is a good example why we should not blindly rely on the Peason's correlation coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- possible start -->\n",
    "df = pd.read_csv(\"Data/4point2.dat\", names=[\"x\", \"y\"])\n",
    "\n",
    "df_uncorr = df[0:20].reset_index(drop=True)\n",
    "df_corr = df[20:40].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** By permutation methods we want to derive distributions of Fisher $r$, Spearman’s and Kendall’s statistics. To do this, first we take the uncorrelated data set and randomly assign $x$-values to $y$-values to make new pairs. This should give the range of values of the test statistic which are consistent with there being no correlation. There are $20!$ distinct permutations for even this little dataset. Compute your results for 1000 of these at random. (This is achieved by sampling without replacement from the set of 20 $X$'s and assigning each one in order to the set of $Y$'s) For each of the 1000 random samples compute their Fisher $r$ value, Spearman’s and Kendall’s statistics and plot their cumulative distributions. Explain what the obtained plots tell. **20 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Now take the correlated set of data and do the same as **b)**. Interprete the results with comparing the plots of uncorrelated datasets. **10 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PCA using covariance [50 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will redo the Principal Component Analysis (PCA) as presented by Francis & Wills (1999) on a set of quasar data. The paper can be downloaded from this [link](https://arxiv.org/pdf/astro-ph/9905079). \n",
    "The data is available on the repository as datafile: `Data/quasar.dat`. The main result from the paper is shown in their Table 3:\n",
    "\n",
    "![Table 3 from Francis and Wills (1999)](images/table_3.png)\n",
    "\n",
    "We now carry through a PCA on the data of the quasar sample given in Francis & Wills (1999)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Read the paper! Remove data rows that have missing data. Create a table of the original data and compute mean value and standard deviation of each column. **5 Points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['logL 1216', 'alpha', 'logFWHM/Hbeta', 'FeII/Hbeta', 'logEW/OIII', 'logFWHM/CIII', 'logEW/Lalpha', 'logEW/CIV', 'CIV/Lalpha', 'logEW/CIII', 'SiIII/CIII', 'NV Lalpha', '1400A Lalpha']\n",
    "quasar = pd.read_csv('Data/quasar.dat', skiprows=6, skipfooter=9, names=names, sep='\\s+', usecols=[1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "\n",
    "<!--  cleaning of data  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Take the original data and put it into normalized or weighted form, so that the effect of different units is effectively removed. Normalize by the standard deviation! **5 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Visually inspect the data after the normalization by plotting each column ($x$: data index, $y$: data value). Confirm (by eye) that each component is about normally distributed. **10 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Construct the covariance matrix. This is a $13\\times13$ symmetric matrix. **10 Points**\n",
    "\n",
    "$$ C_{ij} = \\sigma_i \\sigma_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Compute the eigenvalues and eigenvectors of the covariance matrix. Plot the Eigenvalues against their number (index). Recreate Table 3 from Francis & Wills (1999). **10 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Compute errors of the eigenvalues with a bootstrap analysis or jackknife. Use sample size of 10000. Plot the distributions for the first 5 eigenvalues. **10 Points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "title": ""
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
